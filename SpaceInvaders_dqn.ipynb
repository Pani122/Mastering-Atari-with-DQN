{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mpieQAoS_3q",
        "outputId": "420f3a0d-2882-4557-a8ec-8936b76886ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.9 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym[all] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (0.0.8)\n",
            "Collecting lz4>=3.1.0 (from gym[all])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.34.2)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.10.0.84)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.2.1)\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gym[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl.metadata (669 bytes)\n",
            "Collecting box2d-py==2.3.5 (from gym[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mujoco==2.2.0 (from gym[all])\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting pygame==2.1.0 (from gym[all])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting ale-py~=0.7.5 (from gym[all])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pytest==7.0.1 (from gym[all])\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (3.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.2.0->gym[all])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (24.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (1.5.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.0.1->gym[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[all]) (6.4.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[all]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (3.0.11)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.17.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gym[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
            "Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376131 sha256=9c5961451ae18b108aea0a1ba28e4e368cf98be74387a821741062fea95985a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: glfw, box2d-py, pygame, py, mujoco, lz4, fasteners, ale-py, pytest, mujoco-py\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.0\n",
            "    Uninstalling pygame-2.6.0:\n",
            "      Successfully uninstalled pygame-2.6.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "Successfully installed ale-py-0.7.5 box2d-py-2.3.5 fasteners-0.19 glfw-2.7.0 lz4-4.3.3 mujoco-2.2.0 mujoco-py-2.1.2.14 py-1.11.0 pygame-2.1.0 pytest-7.0.1\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.66.5)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2024.7.4)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=35009bdc67b2b8e339d7f46437193c2b2724a27ac28255b5c5a4ca354fa60773\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q swig\n",
        "!pip3 install gym[all]\n",
        "!pip3 install gym[accept-rom-license]\n",
        "!pip install gym pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kj3y9ByS_3r",
        "outputId": "7e33004e-ddfc-41a9-d2bf-d57ea0d59033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n",
            "Fetched 7,813 kB in 3s (2,491 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123594 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "!apt-get install xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUI6hrbjS_3s",
        "outputId": "99456f51-e7c5-467d-bfef-b0ccb7b8659d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data=''''''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = RecordVideo(env, './video', episode_trigger = lambda episode_number: True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EfGdt28XS_3s"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "from gym.wrappers import  AtariPreprocessing, FrameStack\n",
        "from gym.wrappers.frame_stack import LazyFrames\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cK0KqNJnS_3t"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "        def __init__(self, channels, num_actions):\n",
        "            super(DQN, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(channels, 32, kernel_size=8, stride=4)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "            self.fc = nn.Linear(64*7*7, 512)  # Adjusted input size\n",
        "            self.head = nn.Linear(512, num_actions)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.lrelu = nn.LeakyReLU(0.01)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.float()\n",
        "            x = self.relu(self.conv1(x))\n",
        "            x = self.relu(self.conv2(x))\n",
        "            x = self.relu(self.conv3(x))\n",
        "            # Flatten the tensor before the fully connected layer\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.lrelu(self.fc(x))\n",
        "            q = self.head(x)\n",
        "            return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "18YOzruqS_3t"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, batch_size):\n",
        "        self.capacity = capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.position = 0\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "        else:\n",
        "            self.buffer[self.position] = experience\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.buffer, k=self.batch_size)\n",
        "        states = torch.cat([e.state for e in experiences if e is not None]).float().cuda()\n",
        "        next_states = torch.cat([e.next_state for e in experiences if e is not None]).float().cuda()\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().cuda()\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().cuda()\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().cuda()\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GQmzqYZ8S_3t"
      },
      "outputs": [],
      "source": [
        "class AgentDQN():\n",
        "    def __init__(self, env):\n",
        "        self.environment = env\n",
        "        self.input_channels = 4\n",
        "        self.action_space_size = self.environment.action_space.n\n",
        "        print('Using Deep Q Network')\n",
        "\n",
        "        self.online_network = DQN(self.input_channels, self.action_space_size).cuda() if use_cuda else DQN(self.input_channels, self.action_space_size)\n",
        "        self.target_network = DQN(self.input_channels, self.action_space_size).cuda() if use_cuda else DQN(self.input_channels, self.action_space_size)\n",
        "\n",
        "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.learning_frequency = 8\n",
        "        self.total_training_steps = 1000000\n",
        "        self.learning_start_threshold = 10000\n",
        "        self.batch_size = 32\n",
        "        self.target_update_frequency = 1000\n",
        "        self.display_frequency = 25\n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.online_network.parameters(), lr=1e-4)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.epsilon_min = 0.025\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_decay_steps = 60000\n",
        "        self.metrics = {'steps': [], 'rewards': []}\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(100000, self.batch_size)\n",
        "        self.test_actions = []\n",
        "\n",
        "\n",
        "    def save_model(self, path):\n",
        "        print(f'Saving model to {path}')\n",
        "        model_state = {'online': self.online_network.state_dict(), 'target': self.target_network.state_dict()}\n",
        "        torch.save(model_state, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        print(f'Loading model from {path}')\n",
        "        if use_cuda:\n",
        "            self.online_network.load_state_dict(torch.load(path)['online'])\n",
        "        else:\n",
        "            self.online_network.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage)['online'])\n",
        "\n",
        "    def compute_epsilon(self, step):\n",
        "        if step > self.epsilon_decay_steps:\n",
        "            return 0\n",
        "        else:\n",
        "            return self.epsilon_min + (self.epsilon_max - self.epsilon_min) * ((self.epsilon_decay_steps - step) / self.epsilon_decay_steps)\n",
        "\n",
        "\n",
        "    def select_action(self, state, evaluate=False):\n",
        "        if evaluate:\n",
        "            if not isinstance(state, torch.Tensor):  # Check if state is not already a tensor\n",
        "                state = torch.from_numpy(state).unsqueeze(0)\n",
        "            state = state.cuda() if use_cuda else state\n",
        "            with torch.no_grad():\n",
        "                action = self.online_network(state).max(1)[1].item()\n",
        "        else:\n",
        "            if random.random() > self.compute_epsilon(self.current_step):\n",
        "                with torch.no_grad():\n",
        "                    action = self.online_network(state).max(1)[1].item()\n",
        "            else:\n",
        "                action = random.randrange(self.action_space_size)\n",
        "        return action\n",
        "\n",
        "    def optimize_model(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample()\n",
        "\n",
        "        next_q_values = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        rewards = rewards.clamp(-2.0)\n",
        "\n",
        "        current_q_values = self.online_network(states).gather(1, actions)\n",
        "        expected_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = F.mse_loss(current_q_values, expected_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def train_agent(self):\n",
        "        best_reward = 0\n",
        "        episode_number = 1\n",
        "        reward_history = []\n",
        "        loss_history = []\n",
        "\n",
        "        while self.current_step < self.total_training_steps:\n",
        "            state = np.array(self.environment.reset())\n",
        "            state = torch.from_numpy(state).unsqueeze(0)\n",
        "            state = state.cuda() if use_cuda else state\n",
        "\n",
        "            done = False\n",
        "            episode_rewards = []\n",
        "            episode_losses = []\n",
        "\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done, _ = self.environment.step(action)\n",
        "                next_state = np.array(next_state)\n",
        "\n",
        "                next_state = torch.from_numpy(next_state).unsqueeze(0)\n",
        "                next_state = next_state.cuda() if use_cuda else next_state\n",
        "\n",
        "                episode_rewards.append(reward)\n",
        "                reward_history.append(reward)\n",
        "\n",
        "                self.replay_buffer.store(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "                if self.current_step > self.learning_start_threshold and self.current_step % self.learning_frequency == 0:\n",
        "                    loss = self.optimize_model()\n",
        "                    episode_losses.append(loss)\n",
        "                    loss_history.append(loss)\n",
        "\n",
        "                if self.current_step > self.learning_start_threshold and self.current_step % self.target_update_frequency == 0:\n",
        "                    self.target_network.load_state_dict(self.online_network.state_dict())\n",
        "\n",
        "                self.current_step += 1\n",
        "\n",
        "            avg_loss = sum(episode_losses) / len(episode_losses) if episode_losses else 0\n",
        "            print(f'Episode: {episode_number} | Steps: {self.current_step}/{self.total_training_steps} | Avg Reward: {sum(episode_rewards)} | Loss: {avg_loss:.4f}', end='\\r')\n",
        "\n",
        "            self.metrics['steps'].append(episode_number)\n",
        "            self.metrics['rewards'].append(sum(episode_rewards))\n",
        "\n",
        "            if episode_number % self.display_frequency == 0:\n",
        "                avg_reward = sum(reward_history) / self.display_frequency\n",
        "                avg_loss = sum(loss_history) / len(loss_history) if loss_history else 0\n",
        "                phase = \"Exploring Phase\" if self.current_step < self.epsilon_decay_steps else \"Learning Phase\"\n",
        "\n",
        "                print(f'{phase} | Episode: {episode_number} | Steps: {self.current_step}/{self.total_training_steps} | Epsilon: {self.compute_epsilon(self.current_step):.4f} | Avg Reward: {avg_reward:.2f} | Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "                reward_history = []\n",
        "                loss_history = []\n",
        "\n",
        "            episode_number += 1\n",
        "\n",
        "        self.test_model()\n",
        "        self.plot_training_progress()\n",
        "\n",
        "    def test_model(self):\n",
        "        state = np.array(self.environment.reset(seed=30))\n",
        "\n",
        "        state = torch.from_numpy(state).unsqueeze(0)\n",
        "        state = state.cuda() if use_cuda else state\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "          action = self.select_action(state, evaluate=True)\n",
        "          next_state, _, done, _ = self.environment.step(action)\n",
        "\n",
        "          next_state = np.array(next_state)\n",
        "\n",
        "          next_state = torch.from_numpy(next_state).unsqueeze(0)\n",
        "          next_state = next_state.cuda() if use_cuda else next_state\n",
        "          state = next_state\n",
        "          self.test_actions.append(action)\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        plt.plot(self.metrics['steps'], self.metrics['rewards'])\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('DQN Training Progress')\n",
        "\n",
        "        if not os.path.exists('Plots/DQN'):\n",
        "            os.makedirs('Plots/DQN')\n",
        "        plt.savefig(f'Plots/DQN/{game}_{self.gamma}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wVHNo76fS_3t"
      },
      "outputs": [],
      "source": [
        "def wrap_environment(env, num_stack=4, grayscale=True, scale_obs=False, terminal_on_life_loss=True):\n",
        "    env = AtariPreprocessing(env, grayscale_obs=grayscale, scale_obs=scale_obs, terminal_on_life_loss=terminal_on_life_loss)\n",
        "    env = wrap_env(FrameStack(env, num_stack=num_stack))\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7yz9KFsS_3u",
        "outputId": "9d0e2cf0-590f-46d0-aa19-0beb8f8e705b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Using Deep Q Network\n",
            "Exploring Phase | Episode: 25 | Steps: 4785/1000000 | Epsilon: 0.9222 | Avg Reward: 47.20 | Avg Loss: 0.0000\n",
            "Exploring Phase | Episode: 50 | Steps: 9948/1000000 | Epsilon: 0.8383 | Avg Reward: 78.80 | Avg Loss: 0.0000\n",
            "Exploring Phase | Episode: 75 | Steps: 15152/1000000 | Epsilon: 0.7538 | Avg Reward: 52.20 | Avg Loss: 8.6527\n",
            "Exploring Phase | Episode: 100 | Steps: 19193/1000000 | Epsilon: 0.6881 | Avg Reward: 40.20 | Avg Loss: 4.8375\n",
            "Exploring Phase | Episode: 125 | Steps: 24160/1000000 | Epsilon: 0.6074 | Avg Reward: 49.80 | Avg Loss: 5.3869\n",
            "Exploring Phase | Episode: 150 | Steps: 30058/1000000 | Epsilon: 0.5116 | Avg Reward: 80.80 | Avg Loss: 8.1782\n",
            "Exploring Phase | Episode: 175 | Steps: 35309/1000000 | Epsilon: 0.4262 | Avg Reward: 63.20 | Avg Loss: 16.2285\n",
            "Exploring Phase | Episode: 200 | Steps: 39849/1000000 | Epsilon: 0.3525 | Avg Reward: 62.00 | Avg Loss: 13.1755\n",
            "Exploring Phase | Episode: 225 | Steps: 44247/1000000 | Epsilon: 0.2810 | Avg Reward: 40.40 | Avg Loss: 19.2792\n",
            "Exploring Phase | Episode: 250 | Steps: 49730/1000000 | Epsilon: 0.1919 | Avg Reward: 61.60 | Avg Loss: 13.8349\n",
            "Exploring Phase | Episode: 275 | Steps: 55050/1000000 | Epsilon: 0.1054 | Avg Reward: 54.20 | Avg Loss: 17.5765\n",
            "Exploring Phase | Episode: 300 | Steps: 59761/1000000 | Epsilon: 0.0289 | Avg Reward: 58.80 | Avg Loss: 20.7713\n",
            "Learning Phase | Episode: 325 | Steps: 64725/1000000 | Epsilon: 0.0000 | Avg Reward: 57.40 | Avg Loss: 15.2208\n",
            "Learning Phase | Episode: 350 | Steps: 70121/1000000 | Epsilon: 0.0000 | Avg Reward: 74.00 | Avg Loss: 28.8791\n",
            "Learning Phase | Episode: 375 | Steps: 75123/1000000 | Epsilon: 0.0000 | Avg Reward: 68.60 | Avg Loss: 24.9224\n",
            "Learning Phase | Episode: 400 | Steps: 81422/1000000 | Epsilon: 0.0000 | Avg Reward: 70.60 | Avg Loss: 19.6458\n",
            "Learning Phase | Episode: 425 | Steps: 87817/1000000 | Epsilon: 0.0000 | Avg Reward: 87.80 | Avg Loss: 29.4213\n",
            "Learning Phase | Episode: 450 | Steps: 93053/1000000 | Epsilon: 0.0000 | Avg Reward: 62.20 | Avg Loss: 29.3406\n",
            "Learning Phase | Episode: 475 | Steps: 98281/1000000 | Epsilon: 0.0000 | Avg Reward: 64.00 | Avg Loss: 31.8414\n",
            "Learning Phase | Episode: 500 | Steps: 103632/1000000 | Epsilon: 0.0000 | Avg Reward: 80.20 | Avg Loss: 28.5499\n",
            "Learning Phase | Episode: 525 | Steps: 109022/1000000 | Epsilon: 0.0000 | Avg Reward: 73.80 | Avg Loss: 41.3503\n",
            "Learning Phase | Episode: 550 | Steps: 113296/1000000 | Epsilon: 0.0000 | Avg Reward: 47.80 | Avg Loss: 54.0020\n",
            "Learning Phase | Episode: 575 | Steps: 118145/1000000 | Epsilon: 0.0000 | Avg Reward: 61.40 | Avg Loss: 46.5259\n",
            "Learning Phase | Episode: 600 | Steps: 123990/1000000 | Epsilon: 0.0000 | Avg Reward: 92.80 | Avg Loss: 48.6516\n",
            "Learning Phase | Episode: 625 | Steps: 128859/1000000 | Epsilon: 0.0000 | Avg Reward: 79.80 | Avg Loss: 58.7474\n",
            "Learning Phase | Episode: 650 | Steps: 133885/1000000 | Epsilon: 0.0000 | Avg Reward: 100.80 | Avg Loss: 52.1955\n",
            "Learning Phase | Episode: 675 | Steps: 138584/1000000 | Epsilon: 0.0000 | Avg Reward: 86.40 | Avg Loss: 55.0841\n",
            "Learning Phase | Episode: 700 | Steps: 143073/1000000 | Epsilon: 0.0000 | Avg Reward: 79.40 | Avg Loss: 54.6244\n",
            "Learning Phase | Episode: 725 | Steps: 148015/1000000 | Epsilon: 0.0000 | Avg Reward: 78.40 | Avg Loss: 48.7547\n",
            "Learning Phase | Episode: 750 | Steps: 153103/1000000 | Epsilon: 0.0000 | Avg Reward: 96.00 | Avg Loss: 52.9169\n",
            "Learning Phase | Episode: 775 | Steps: 158882/1000000 | Epsilon: 0.0000 | Avg Reward: 94.40 | Avg Loss: 61.4861\n",
            "Learning Phase | Episode: 800 | Steps: 163880/1000000 | Epsilon: 0.0000 | Avg Reward: 88.80 | Avg Loss: 53.6296\n",
            "Learning Phase | Episode: 825 | Steps: 168492/1000000 | Epsilon: 0.0000 | Avg Reward: 79.40 | Avg Loss: 62.3200\n",
            "Learning Phase | Episode: 850 | Steps: 173006/1000000 | Epsilon: 0.0000 | Avg Reward: 88.00 | Avg Loss: 40.7188\n",
            "Learning Phase | Episode: 875 | Steps: 178399/1000000 | Epsilon: 0.0000 | Avg Reward: 108.20 | Avg Loss: 59.9321\n",
            "Learning Phase | Episode: 900 | Steps: 184401/1000000 | Epsilon: 0.0000 | Avg Reward: 106.60 | Avg Loss: 54.8779\n",
            "Learning Phase | Episode: 925 | Steps: 189776/1000000 | Epsilon: 0.0000 | Avg Reward: 93.20 | Avg Loss: 53.5115\n",
            "Learning Phase | Episode: 950 | Steps: 194539/1000000 | Epsilon: 0.0000 | Avg Reward: 86.00 | Avg Loss: 60.0445\n",
            "Learning Phase | Episode: 975 | Steps: 199181/1000000 | Epsilon: 0.0000 | Avg Reward: 84.60 | Avg Loss: 53.6796\n",
            "Learning Phase | Episode: 1000 | Steps: 203660/1000000 | Epsilon: 0.0000 | Avg Reward: 88.40 | Avg Loss: 47.7236\n",
            "Learning Phase | Episode: 1025 | Steps: 208226/1000000 | Epsilon: 0.0000 | Avg Reward: 84.40 | Avg Loss: 65.1206\n",
            "Learning Phase | Episode: 1050 | Steps: 212941/1000000 | Epsilon: 0.0000 | Avg Reward: 77.20 | Avg Loss: 57.1175\n",
            "Learning Phase | Episode: 1075 | Steps: 217875/1000000 | Epsilon: 0.0000 | Avg Reward: 84.80 | Avg Loss: 48.7469\n",
            "Learning Phase | Episode: 1100 | Steps: 223460/1000000 | Epsilon: 0.0000 | Avg Reward: 84.00 | Avg Loss: 48.4558\n",
            "Learning Phase | Episode: 1125 | Steps: 229574/1000000 | Epsilon: 0.0000 | Avg Reward: 117.60 | Avg Loss: 41.6403\n",
            "Learning Phase | Episode: 1150 | Steps: 234797/1000000 | Epsilon: 0.0000 | Avg Reward: 93.00 | Avg Loss: 40.8507\n",
            "Learning Phase | Episode: 1175 | Steps: 240845/1000000 | Epsilon: 0.0000 | Avg Reward: 116.80 | Avg Loss: 40.4940\n",
            "Learning Phase | Episode: 1200 | Steps: 246882/1000000 | Epsilon: 0.0000 | Avg Reward: 98.40 | Avg Loss: 43.7528\n",
            "Learning Phase | Episode: 1225 | Steps: 253190/1000000 | Epsilon: 0.0000 | Avg Reward: 100.20 | Avg Loss: 50.7060\n",
            "Learning Phase | Episode: 1250 | Steps: 260670/1000000 | Epsilon: 0.0000 | Avg Reward: 116.60 | Avg Loss: 39.1598\n",
            "Learning Phase | Episode: 1275 | Steps: 268462/1000000 | Epsilon: 0.0000 | Avg Reward: 130.40 | Avg Loss: 43.3618\n",
            "Learning Phase | Episode: 1300 | Steps: 275187/1000000 | Epsilon: 0.0000 | Avg Reward: 110.20 | Avg Loss: 52.8559\n",
            "Learning Phase | Episode: 1325 | Steps: 282476/1000000 | Epsilon: 0.0000 | Avg Reward: 133.00 | Avg Loss: 53.9933\n",
            "Learning Phase | Episode: 1350 | Steps: 289410/1000000 | Epsilon: 0.0000 | Avg Reward: 111.00 | Avg Loss: 48.0195\n",
            "Learning Phase | Episode: 1375 | Steps: 294958/1000000 | Epsilon: 0.0000 | Avg Reward: 80.40 | Avg Loss: 48.0546\n",
            "Learning Phase | Episode: 1400 | Steps: 301836/1000000 | Epsilon: 0.0000 | Avg Reward: 108.80 | Avg Loss: 49.3092\n",
            "Learning Phase | Episode: 1425 | Steps: 308811/1000000 | Epsilon: 0.0000 | Avg Reward: 110.00 | Avg Loss: 51.0000\n",
            "Learning Phase | Episode: 1450 | Steps: 314329/1000000 | Epsilon: 0.0000 | Avg Reward: 84.40 | Avg Loss: 53.8705\n",
            "Learning Phase | Episode: 1475 | Steps: 320303/1000000 | Epsilon: 0.0000 | Avg Reward: 102.60 | Avg Loss: 50.3065\n",
            "Learning Phase | Episode: 1500 | Steps: 327367/1000000 | Epsilon: 0.0000 | Avg Reward: 96.80 | Avg Loss: 48.2580\n",
            "Learning Phase | Episode: 1525 | Steps: 333657/1000000 | Epsilon: 0.0000 | Avg Reward: 104.20 | Avg Loss: 39.3887\n",
            "Learning Phase | Episode: 1550 | Steps: 339469/1000000 | Epsilon: 0.0000 | Avg Reward: 65.00 | Avg Loss: 33.0829\n",
            "Learning Phase | Episode: 1575 | Steps: 345297/1000000 | Epsilon: 0.0000 | Avg Reward: 83.60 | Avg Loss: 43.2714\n",
            "Learning Phase | Episode: 1600 | Steps: 352192/1000000 | Epsilon: 0.0000 | Avg Reward: 104.60 | Avg Loss: 39.0729\n",
            "Learning Phase | Episode: 1625 | Steps: 358595/1000000 | Epsilon: 0.0000 | Avg Reward: 119.00 | Avg Loss: 36.0057\n",
            "Learning Phase | Episode: 1650 | Steps: 365994/1000000 | Epsilon: 0.0000 | Avg Reward: 110.60 | Avg Loss: 39.3198\n",
            "Learning Phase | Episode: 1675 | Steps: 374207/1000000 | Epsilon: 0.0000 | Avg Reward: 109.00 | Avg Loss: 35.0376\n",
            "Learning Phase | Episode: 1700 | Steps: 381388/1000000 | Epsilon: 0.0000 | Avg Reward: 104.00 | Avg Loss: 40.0477\n",
            "Learning Phase | Episode: 1725 | Steps: 388163/1000000 | Epsilon: 0.0000 | Avg Reward: 111.80 | Avg Loss: 34.6698\n",
            "Learning Phase | Episode: 1750 | Steps: 394497/1000000 | Epsilon: 0.0000 | Avg Reward: 110.40 | Avg Loss: 33.0265\n",
            "Learning Phase | Episode: 1775 | Steps: 402672/1000000 | Epsilon: 0.0000 | Avg Reward: 137.80 | Avg Loss: 29.6875\n",
            "Learning Phase | Episode: 1800 | Steps: 410973/1000000 | Epsilon: 0.0000 | Avg Reward: 156.60 | Avg Loss: 33.7892\n",
            "Learning Phase | Episode: 1825 | Steps: 420728/1000000 | Epsilon: 0.0000 | Avg Reward: 178.80 | Avg Loss: 34.2252\n",
            "Learning Phase | Episode: 1850 | Steps: 428555/1000000 | Epsilon: 0.0000 | Avg Reward: 156.00 | Avg Loss: 43.1646\n",
            "Learning Phase | Episode: 1875 | Steps: 436489/1000000 | Epsilon: 0.0000 | Avg Reward: 130.60 | Avg Loss: 35.3775\n",
            "Learning Phase | Episode: 1900 | Steps: 444445/1000000 | Epsilon: 0.0000 | Avg Reward: 136.40 | Avg Loss: 34.7670\n",
            "Learning Phase | Episode: 1925 | Steps: 452829/1000000 | Epsilon: 0.0000 | Avg Reward: 127.80 | Avg Loss: 28.8895\n",
            "Learning Phase | Episode: 1950 | Steps: 462538/1000000 | Epsilon: 0.0000 | Avg Reward: 190.00 | Avg Loss: 20.4811\n",
            "Learning Phase | Episode: 1975 | Steps: 472877/1000000 | Epsilon: 0.0000 | Avg Reward: 178.00 | Avg Loss: 27.0814\n",
            "Learning Phase | Episode: 2000 | Steps: 481823/1000000 | Epsilon: 0.0000 | Avg Reward: 156.20 | Avg Loss: 33.2617\n",
            "Learning Phase | Episode: 2025 | Steps: 491465/1000000 | Epsilon: 0.0000 | Avg Reward: 165.20 | Avg Loss: 26.4007\n",
            "Learning Phase | Episode: 2050 | Steps: 501232/1000000 | Epsilon: 0.0000 | Avg Reward: 183.60 | Avg Loss: 23.2154\n",
            "Learning Phase | Episode: 2075 | Steps: 510294/1000000 | Epsilon: 0.0000 | Avg Reward: 178.60 | Avg Loss: 27.4695\n",
            "Learning Phase | Episode: 2100 | Steps: 518916/1000000 | Epsilon: 0.0000 | Avg Reward: 146.20 | Avg Loss: 26.1076\n",
            "Learning Phase | Episode: 2125 | Steps: 531321/1000000 | Epsilon: 0.0000 | Avg Reward: 253.00 | Avg Loss: 21.9936\n",
            "Learning Phase | Episode: 2150 | Steps: 542609/1000000 | Epsilon: 0.0000 | Avg Reward: 185.60 | Avg Loss: 22.1085\n",
            "Learning Phase | Episode: 2175 | Steps: 552436/1000000 | Epsilon: 0.0000 | Avg Reward: 174.40 | Avg Loss: 25.5115\n",
            "Learning Phase | Episode: 2200 | Steps: 563388/1000000 | Epsilon: 0.0000 | Avg Reward: 188.80 | Avg Loss: 22.3394\n",
            "Learning Phase | Episode: 2225 | Steps: 572265/1000000 | Epsilon: 0.0000 | Avg Reward: 163.00 | Avg Loss: 21.3227\n",
            "Learning Phase | Episode: 2250 | Steps: 582406/1000000 | Epsilon: 0.0000 | Avg Reward: 207.00 | Avg Loss: 19.2896\n",
            "Learning Phase | Episode: 2275 | Steps: 592764/1000000 | Epsilon: 0.0000 | Avg Reward: 200.60 | Avg Loss: 17.2561\n",
            "Learning Phase | Episode: 2300 | Steps: 602589/1000000 | Epsilon: 0.0000 | Avg Reward: 169.20 | Avg Loss: 22.5603\n",
            "Learning Phase | Episode: 2325 | Steps: 612395/1000000 | Epsilon: 0.0000 | Avg Reward: 178.00 | Avg Loss: 27.4717\n",
            "Learning Phase | Episode: 2350 | Steps: 623822/1000000 | Epsilon: 0.0000 | Avg Reward: 225.60 | Avg Loss: 28.1887\n",
            "Learning Phase | Episode: 2375 | Steps: 635039/1000000 | Epsilon: 0.0000 | Avg Reward: 198.40 | Avg Loss: 22.6287\n",
            "Learning Phase | Episode: 2400 | Steps: 646640/1000000 | Epsilon: 0.0000 | Avg Reward: 182.00 | Avg Loss: 25.4408\n",
            "Learning Phase | Episode: 2425 | Steps: 656847/1000000 | Epsilon: 0.0000 | Avg Reward: 176.00 | Avg Loss: 24.9424\n",
            "Learning Phase | Episode: 2450 | Steps: 668316/1000000 | Epsilon: 0.0000 | Avg Reward: 244.80 | Avg Loss: 25.7751\n",
            "Learning Phase | Episode: 2475 | Steps: 680277/1000000 | Epsilon: 0.0000 | Avg Reward: 191.60 | Avg Loss: 23.8244\n",
            "Learning Phase | Episode: 2500 | Steps: 689484/1000000 | Epsilon: 0.0000 | Avg Reward: 219.00 | Avg Loss: 22.9786\n",
            "Learning Phase | Episode: 2525 | Steps: 698454/1000000 | Epsilon: 0.0000 | Avg Reward: 150.20 | Avg Loss: 24.7367\n",
            "Learning Phase | Episode: 2550 | Steps: 709390/1000000 | Epsilon: 0.0000 | Avg Reward: 253.20 | Avg Loss: 30.8653\n",
            "Learning Phase | Episode: 2575 | Steps: 718482/1000000 | Epsilon: 0.0000 | Avg Reward: 178.20 | Avg Loss: 26.7007\n",
            "Learning Phase | Episode: 2600 | Steps: 729316/1000000 | Epsilon: 0.0000 | Avg Reward: 215.80 | Avg Loss: 26.0215\n",
            "Learning Phase | Episode: 2625 | Steps: 740727/1000000 | Epsilon: 0.0000 | Avg Reward: 235.20 | Avg Loss: 27.9453\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    # Setup the initial environment for training\n",
        "    game = 'SpaceInvaders'\n",
        "    env_name = 'SpaceInvadersNoFrameskip-v4'\n",
        "    env = gym.make(env_name)\n",
        "    env = wrap_environment(env, grayscale=True, scale_obs=False, terminal_on_life_loss=True)\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "    agent = AgentDQN(env)\n",
        "    agent.train_agent()\n",
        "    env.close()\n",
        "\n",
        "    # Setup display for rendering the environment\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "\n",
        "    # Setup a new environment for testing\n",
        "    test_env = gym.make(env_name)\n",
        "    test_env = wrap_environment(test_env, grayscale=True, scale_obs=True, terminal_on_life_loss=False)\n",
        "\n",
        "    # Start testing the trained agent\n",
        "    state = torch.from_numpy(np.array(test_env.reset(seed=30))).unsqueeze(0)\n",
        "    state = state.cuda() if use_cuda else state\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step_index = 0\n",
        "\n",
        "    while not done:\n",
        "        test_env.render()\n",
        "        action = agent.test_actions[step_index]\n",
        "        step_index += 1\n",
        "        next_state, reward, done, _ = test_env.step(action)\n",
        "\n",
        "        next_state = torch.from_numpy(np.array(next_state)).unsqueeze(0)\n",
        "        next_state = next_state.cuda() if use_cuda else next_state\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f'Total reward in this episode: {total_reward}')\n",
        "    test_env.close()\n",
        "\n",
        "    # Display the video of the session\n",
        "    show_video()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}